#!/usr/bin/env python3
"""
ğŸ§  ì‹ ê²½ë§ ê¸°ë°˜ ì½”ë“œ ìµœì í™” ì—”ì§„
Neural Network-Powered Code Optimization & Analysis Engine
"""

import asyncio
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, asdict
import json
import hashlib
import uuid
import ast
import inspect
import re
import os
import subprocess
from pathlib import Path
import threading
import multiprocessing
import time
from collections import defaultdict, deque
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset, TensorDataset
from torch.nn.utils.rnn import pad_sequence
import transformers
from transformers import (
    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,
    GPT2LMHeadModel, GPT2Tokenizer, CodeBERTTokenizer, RobertaModel,
    T5ForConditionalGeneration, T5Tokenizer, BartForConditionalGeneration
)
import tokenizers
from tokenizers import Tokenizer
import tree_sitter
from tree_sitter import Language, Parser
import pygments
from pygments.lexers import get_lexer_by_name
from pygments.token import Token
import radon
from radon.complexity import cc_visit
from radon.metrics import mi_visit, h_visit
import bandit
from bandit.core import manager
import pylint
from pylint.lint import PyLinter
import flake8
from flake8.api import legacy as flake8_legacy
import mypy
from mypy import api as mypy_api
import black
import isort
import autopep8
import pycodestyle
import pydocstyle
import vulture
import mccabe
import lizard
import cloc
import requests
import openai
from openai import OpenAI
import anthropic
import google.generativeai as genai
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.cluster import DBSCAN, KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler, LabelEncoder
import pandas as pd
import networkx as nx
from scipy import stats
import redis
import elasticsearch
from elasticsearch import Elasticsearch
import prometheus_client
from prometheus_client import Counter, Histogram, Gauge
import mlflow
import wandb
import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class CodeAnalysisResult:
    """ì½”ë“œ ë¶„ì„ ê²°ê³¼"""
    file_path: str
    language: str
    lines_of_code: int
    cyclomatic_complexity: float
    maintainability_index: float
    technical_debt_ratio: float
    security_score: float
    performance_score: float
    readability_score: float
    test_coverage: float
    code_smells: List[Dict[str, Any]]
    vulnerabilities: List[Dict[str, Any]]
    optimization_suggestions: List[Dict[str, Any]]
    estimated_refactoring_time: int  # minutes
    quality_gate_status: str  # "passed", "warning", "failed"

@dataclass
class OptimizationRecommendation:
    """ìµœì í™” ê¶Œì¥ì‚¬í•­"""
    recommendation_id: str
    priority: str  # "critical", "high", "medium", "low"
    category: str  # "performance", "security", "maintainability", "readability"
    title: str
    description: str
    code_location: Dict[str, Any]  # file, line, column
    original_code: str
    optimized_code: str
    expected_improvement: Dict[str, float]  # metric -> improvement %
    implementation_effort: str  # "trivial", "easy", "moderate", "hard"
    confidence_score: float  # 0.0 - 1.0
    ai_reasoning: str

@dataclass
class CodeMetrics:
    """ì½”ë“œ ë©”íŠ¸ë¦­"""
    file_path: str
    timestamp: datetime
    language: str
    loc: int  # Lines of Code
    sloc: int  # Source Lines of Code
    comments: int
    blanks: int
    complexity: Dict[str, float]
    maintainability: Dict[str, float]
    readability: Dict[str, float]
    performance: Dict[str, float]
    security: Dict[str, float]
    test_metrics: Dict[str, float]
    dependencies: List[str]
    code_patterns: List[str]

class NeuralCodeOptimizer:
    """ì‹ ê²½ë§ ê¸°ë°˜ ì½”ë“œ ìµœì í™” ì—”ì§„"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.models = {}
        self.tokenizers = {}
        self.parsers = {}
        
        # ì½”ë“œ ë¶„ì„ ë„êµ¬
        self.static_analyzers = {}
        self.dynamic_analyzers = {}
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°
        self.metrics_collector = CodeMetricsCollector()
        
        # ìµœì í™” ì—”ì§„ë“¤
        self.performance_optimizer = PerformanceOptimizer()
        self.security_optimizer = SecurityOptimizer()
        self.readability_optimizer = ReadabilityOptimizer()
        self.maintainability_optimizer = MaintainabilityOptimizer()
        
        # AI ëª¨ë¸ë“¤
        self.code_understanding_model = None
        self.code_generation_model = None
        self.vulnerability_detection_model = None
        self.performance_prediction_model = None
        
        # í•™ìŠµ ë°ì´í„°
        self.code_corpus = []
        self.optimization_history = []
        
    async def initialize(self):
        """ì‹ ê²½ë§ ì½”ë“œ ìµœì í™” ì—”ì§„ ì´ˆê¸°í™”"""
        logger.info("ğŸ§  ì‹ ê²½ë§ ì½”ë“œ ìµœì í™” ì—”ì§„ ì´ˆê¸°í™”...")
        
        # AI ëª¨ë¸ ë¡œë“œ
        await self._load_ai_models()
        
        # ì½”ë“œ íŒŒì„œ ì´ˆê¸°í™”
        await self._initialize_code_parsers()
        
        # ì •ì  ë¶„ì„ ë„êµ¬ ì„¤ì •
        await self._setup_static_analyzers()
        
        # ë™ì  ë¶„ì„ ë„êµ¬ ì„¤ì •
        await self._setup_dynamic_analyzers()
        
        # ìµœì í™” ì—”ì§„ ì´ˆê¸°í™”
        await self._initialize_optimization_engines()
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° ì‹œì‘
        await self.metrics_collector.start()
        
        logger.info("âœ… ì‹ ê²½ë§ ì½”ë“œ ìµœì í™” ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def _load_ai_models(self):
        """AI ëª¨ë¸ ë¡œë“œ"""
        
        # CodeBERT - ì½”ë“œ ì´í•´
        self.tokenizers['codebert'] = AutoTokenizer.from_pretrained('microsoft/codebert-base')
        self.models['codebert'] = AutoModel.from_pretrained('microsoft/codebert-base')
        
        # CodeT5 - ì½”ë“œ ìƒì„± ë° ë²ˆì—­
        self.tokenizers['codet5'] = T5Tokenizer.from_pretrained('Salesforce/codet5-base')
        self.models['codet5'] = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')
        
        # GraphCodeBERT - ì½”ë“œ êµ¬ì¡° ì´í•´
        self.tokenizers['graphcodebert'] = AutoTokenizer.from_pretrained('microsoft/graphcodebert-base')
        self.models['graphcodebert'] = AutoModel.from_pretrained('microsoft/graphcodebert-base')
        
        # ì»¤ìŠ¤í…€ ëª¨ë¸ë“¤
        self.code_understanding_model = CodeUnderstandingTransformer()
        self.code_generation_model = CodeGenerationTransformer()
        self.vulnerability_detection_model = VulnerabilityDetectionModel()
        self.performance_prediction_model = PerformancePredictionModel()
        
        # ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        await self._load_pretrained_weights()
        
        logger.info("ğŸ¤– AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    async def _initialize_code_parsers(self):
        """ì½”ë“œ íŒŒì„œ ì´ˆê¸°í™”"""
        
        # Tree-sitter íŒŒì„œë“¤
        try:
            # ì–¸ì–´ë³„ íŒŒì„œ ì„¤ì •
            cpp_language = Language('build/languages.so', 'cpp')
            python_language = Language('build/languages.so', 'python')
            javascript_language = Language('build/languages.so', 'javascript')
            
            self.parsers['cpp'] = Parser()
            self.parsers['cpp'].set_language(cpp_language)
            
            self.parsers['python'] = Parser()
            self.parsers['python'].set_language(python_language)
            
            self.parsers['javascript'] = Parser()
            self.parsers['javascript'].set_language(javascript_language)
            
        except Exception as e:
            logger.warning(f"Tree-sitter íŒŒì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ë°±ì—… íŒŒì„œ ì‚¬ìš©
            await self._setup_backup_parsers()
        
        logger.info("ğŸŒ³ ì½”ë“œ íŒŒì„œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def analyze_code_comprehensively(self, 
                                         code_content: str,
                                         file_path: str,
                                         language: str) -> CodeAnalysisResult:
        """ì¢…í•©ì ì¸ ì½”ë“œ ë¶„ì„"""
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        basic_metrics = await self._collect_basic_metrics(code_content, language)
        
        # ë³µì¡ë„ ë¶„ì„
        complexity_analysis = await self._analyze_complexity(code_content, language)
        
        # ë³´ì•ˆ ë¶„ì„
        security_analysis = await self._analyze_security(code_content, language)
        
        # ì„±ëŠ¥ ë¶„ì„
        performance_analysis = await self._analyze_performance(code_content, language)
        
        # ê°€ë…ì„± ë¶„ì„
        readability_analysis = await self._analyze_readability(code_content, language)
        
        # ìœ ì§€ë³´ìˆ˜ì„± ë¶„ì„
        maintainability_analysis = await self._analyze_maintainability(code_content, language)
        
        # AI ê¸°ë°˜ ì‹¬ì¸µ ë¶„ì„
        ai_analysis = await self._ai_deep_analysis(code_content, language)
        
        # ì½”ë“œ ìŠ¤ë©œ íƒì§€
        code_smells = await self._detect_code_smells(code_content, language)
        
        # ì·¨ì•½ì  íƒì§€
        vulnerabilities = await self._detect_vulnerabilities(code_content, language)
        
        # ìµœì í™” ì œì•ˆ ìƒì„±
        optimization_suggestions = await self._generate_optimization_suggestions(
            code_content, language, {
                'complexity': complexity_analysis,
                'security': security_analysis,
                'performance': performance_analysis,
                'readability': readability_analysis,
                'maintainability': maintainability_analysis,
                'ai_insights': ai_analysis
            }
        )
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_scores = await self._calculate_quality_scores(
            complexity_analysis, security_analysis, performance_analysis,
            readability_analysis, maintainability_analysis
        )
        
        # í’ˆì§ˆ ê²Œì´íŠ¸ ìƒíƒœ ê²°ì •
        quality_gate_status = await self._determine_quality_gate_status(quality_scores)
        
        analysis_result = CodeAnalysisResult(
            file_path=file_path,
            language=language,
            lines_of_code=basic_metrics['loc'],
            cyclomatic_complexity=complexity_analysis['cyclomatic_complexity'],
            maintainability_index=maintainability_analysis['maintainability_index'],
            technical_debt_ratio=maintainability_analysis['technical_debt_ratio'],
            security_score=quality_scores['security_score'],
            performance_score=quality_scores['performance_score'],
            readability_score=quality_scores['readability_score'],
            test_coverage=basic_metrics.get('test_coverage', 0.0),
            code_smells=code_smells,
            vulnerabilities=vulnerabilities,
            optimization_suggestions=optimization_suggestions,
            estimated_refactoring_time=maintainability_analysis['estimated_refactoring_time'],
            quality_gate_status=quality_gate_status
        )
        
        return analysis_result
    
    async def _ai_deep_analysis(self, code_content: str, language: str) -> Dict[str, Any]:
        """AI ê¸°ë°˜ ì‹¬ì¸µ ì½”ë“œ ë¶„ì„"""
        
        # CodeBERTë¡œ ì½”ë“œ ì„ë² ë”© ìƒì„±
        code_embedding = await self._generate_code_embedding(code_content, language)
        
        # ì½”ë“œ íŒ¨í„´ ë¶„ì„
        patterns = await self._analyze_code_patterns(code_content, language, code_embedding)
        
        # ì˜ë¯¸ë¡ ì  ë¶„ì„
        semantic_analysis = await self._semantic_code_analysis(code_content, language)
        
        # ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ ì˜ˆì¸¡
        algorithmic_complexity = await self._predict_algorithmic_complexity(code_content, language)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡
        memory_prediction = await self._predict_memory_usage(code_content, language)
        
        # ì‹¤í–‰ ì‹œê°„ ì˜ˆì¸¡
        runtime_prediction = await self._predict_runtime(code_content, language)
        
        # ì½”ë“œ í’ˆì§ˆ ì˜ˆì¸¡
        quality_prediction = await self._predict_code_quality(code_content, language)
        
        # ë²„ê·¸ ë°œìƒ í™•ë¥  ì˜ˆì¸¡
        bug_probability = await self._predict_bug_probability(code_content, language)
        
        return {
            'code_embedding': code_embedding.tolist(),
            'detected_patterns': patterns,
            'semantic_analysis': semantic_analysis,
            'algorithmic_complexity': algorithmic_complexity,
            'memory_prediction': memory_prediction,
            'runtime_prediction': runtime_prediction,
            'quality_prediction': quality_prediction,
            'bug_probability': bug_probability,
            'ai_confidence': 0.92
        }
    
    async def _generate_code_embedding(self, code_content: str, language: str) -> np.ndarray:
        """ì½”ë“œ ì„ë² ë”© ìƒì„±"""
        
        # CodeBERT í† í¬ë‚˜ì´ì €ë¡œ í† í°í™”
        tokenizer = self.tokenizers['codebert']
        model = self.models['codebert']
        
        # ì½”ë“œ ì „ì²˜ë¦¬
        processed_code = await self._preprocess_code_for_embedding(code_content, language)
        
        # í† í°í™”
        inputs = tokenizer(
            processed_code,
            return_tensors='pt',
            max_length=512,
            truncation=True,
            padding=True
        )
        
        # ì„ë² ë”© ìƒì„±
        with torch.no_grad():
            outputs = model(**inputs)
            # CLS í† í°ì˜ ì„ë² ë”© ì‚¬ìš© (ë¬¸ì¥ ì „ì²´ í‘œí˜„)
            code_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()
        
        return code_embedding
    
    async def _analyze_code_patterns(self, 
                                   code_content: str, 
                                   language: str,
                                   code_embedding: np.ndarray) -> List[Dict[str, Any]]:
        """ì½”ë“œ íŒ¨í„´ ë¶„ì„"""
        
        patterns = []
        
        # ë””ìì¸ íŒ¨í„´ íƒì§€
        design_patterns = await self._detect_design_patterns(code_content, language)
        patterns.extend(design_patterns)
        
        # ì•ˆí‹°íŒ¨í„´ íƒì§€
        anti_patterns = await self._detect_anti_patterns(code_content, language)
        patterns.extend(anti_patterns)
        
        # í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë° íŒ¨í„´
        if language in ['python', 'javascript']:
            functional_patterns = await self._detect_functional_patterns(code_content)
            patterns.extend(functional_patterns)
        
        # ê°ì²´ì§€í–¥ íŒ¨í„´
        if language in ['cpp', 'python', 'java']:
            oop_patterns = await self._detect_oop_patterns(code_content)
            patterns.extend(oop_patterns)
        
        # Arduino/IoT íŠ¹í™” íŒ¨í„´
        if language == 'cpp':
            iot_patterns = await self._detect_iot_patterns(code_content)
            patterns.extend(iot_patterns)
        
        return patterns
    
    async def _detect_iot_patterns(self, code_content: str) -> List[Dict[str, Any]]:
        """Arduino/IoT íŠ¹í™” íŒ¨í„´ íƒì§€"""
        
        patterns = []
        
        # Arduino ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© íŒ¨í„´
        arduino_libs = [
            'WiFi.h', 'ESP8266WiFi.h', 'WiFiClient.h',
            'PubSubClient.h', 'ArduinoJson.h', 'DHT.h',
            'Servo.h', 'SoftwareSerial.h', 'Wire.h'
        ]
        
        for lib in arduino_libs:
            if f'#include <{lib}>' in code_content:
                patterns.append({
                    'type': 'arduino_library',
                    'pattern': f'{lib} usage',
                    'confidence': 0.95,
                    'line_number': await self._find_line_number(code_content, f'#include <{lib}>')
                })
        
        # ì„¼ì„œ ì½ê¸° íŒ¨í„´
        sensor_patterns = [
            r'analogRead\s*\(\s*A?\d+\s*\)',
            r'digitalRead\s*\(\s*\d+\s*\)',
            r'dht\.read\w*\(\)',
            r'sensor\.read\w*\(\)'
        ]
        
        for pattern in sensor_patterns:
            matches = re.finditer(pattern, code_content, re.IGNORECASE)
            for match in matches:
                line_num = code_content[:match.start()].count('\n') + 1
                patterns.append({
                    'type': 'sensor_reading',
                    'pattern': 'Sensor data acquisition',
                    'confidence': 0.90,
                    'line_number': line_num,
                    'code_snippet': match.group()
                })
        
        # í†µì‹  íŒ¨í„´
        if 'WiFi.begin' in code_content:
            patterns.append({
                'type': 'wifi_connection',
                'pattern': 'WiFi connectivity setup',
                'confidence': 0.95,
                'optimization_hint': 'Consider connection retry logic and timeout handling'
            })
        
        if 'client.publish' in code_content or 'mqtt' in code_content.lower():
            patterns.append({
                'type': 'mqtt_communication',
                'pattern': 'MQTT messaging',
                'confidence': 0.90,
                'optimization_hint': 'Implement QoS levels and connection persistence'
            })
        
        # ì „ë ¥ ê´€ë¦¬ íŒ¨í„´
        power_patterns = [
            'ESP.deepSleep',
            'LowPower.powerDown',
            'WiFi.mode(WIFI_OFF)'
        ]
        
        for pattern in power_patterns:
            if pattern in code_content:
                patterns.append({
                    'type': 'power_management',
                    'pattern': 'Energy efficiency optimization',
                    'confidence': 0.85,
                    'benefit': 'Extended battery life'
                })
        
        return patterns
    
    async def generate_optimization_recommendations(self, 
                                                 analysis_result: CodeAnalysisResult) -> List[OptimizationRecommendation]:
        """ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­
        performance_recs = await self.performance_optimizer.generate_recommendations(
            analysis_result
        )
        recommendations.extend(performance_recs)
        
        # ë³´ì•ˆ ìµœì í™” ê¶Œì¥ì‚¬í•­
        security_recs = await self.security_optimizer.generate_recommendations(
            analysis_result
        )
        recommendations.extend(security_recs)
        
        # ê°€ë…ì„± ê°œì„  ê¶Œì¥ì‚¬í•­
        readability_recs = await self.readability_optimizer.generate_recommendations(
            analysis_result
        )
        recommendations.extend(readability_recs)
        
        # ìœ ì§€ë³´ìˆ˜ì„± ê°œì„  ê¶Œì¥ì‚¬í•­
        maintainability_recs = await self.maintainability_optimizer.generate_recommendations(
            analysis_result
        )
        recommendations.extend(maintainability_recs)
        
        # AI ê¸°ë°˜ ì¢…í•© ê¶Œì¥ì‚¬í•­
        ai_recs = await self._generate_ai_recommendations(analysis_result)
        recommendations.extend(ai_recs)
        
        # ìš°ì„ ìˆœìœ„ ì •ë ¬
        recommendations = await self._prioritize_recommendations(recommendations)
        
        return recommendations
    
    async def _generate_ai_recommendations(self, 
                                         analysis_result: CodeAnalysisResult) -> List[OptimizationRecommendation]:
        """AI ê¸°ë°˜ ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        try:
            with open(analysis_result.file_path, 'r', encoding='utf-8') as f:
                code_content = f.read()
        except:
            return recommendations
        
        # GPT-4 ê¸°ë°˜ ì½”ë“œ ë¶„ì„ ë° ìµœì í™” ì œì•ˆ
        ai_suggestions = await self._get_gpt4_suggestions(
            code_content, analysis_result.language
        )
        
        for suggestion in ai_suggestions:
            recommendation = OptimizationRecommendation(
                recommendation_id=f"ai_{uuid.uuid4().hex[:8]}",
                priority=suggestion['priority'],
                category=suggestion['category'],
                title=suggestion['title'],
                description=suggestion['description'],
                code_location=suggestion['location'],
                original_code=suggestion['original_code'],
                optimized_code=suggestion['optimized_code'],
                expected_improvement=suggestion['expected_improvement'],
                implementation_effort=suggestion['effort'],
                confidence_score=suggestion['confidence'],
                ai_reasoning=suggestion['reasoning']
            )
            recommendations.append(recommendation)
        
        return recommendations
    
    async def _get_gpt4_suggestions(self, 
                                  code_content: str, 
                                  language: str) -> List[Dict[str, Any]]:
        """GPT-4ë¥¼ í†µí•œ ì½”ë“œ ìµœì í™” ì œì•ˆ"""
        
        client = OpenAI(api_key=self.config.get('openai_api_key'))
        
        prompt = f"""
        ë‹¤ìŒ {language} ì½”ë“œë¥¼ ë¶„ì„í•˜ê³  ìµœì í™” ì œì•ˆì„ í•´ì£¼ì„¸ìš”:

        ```{language}
        {code_content}
        ```

        ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:
        1. ì„±ëŠ¥ ìµœì í™” (ë©”ëª¨ë¦¬, ì†ë„, ì „ë ¥ íš¨ìœ¨ì„±)
        2. ë³´ì•ˆ ê°•í™” (ì·¨ì•½ì , ë°ì´í„° ë³´í˜¸)
        3. ì½”ë“œ í’ˆì§ˆ (ê°€ë…ì„±, ìœ ì§€ë³´ìˆ˜ì„±)
        4. Arduino/IoT íŠ¹í™” ìµœì í™”

        ê° ì œì•ˆì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        - ìš°ì„ ìˆœìœ„: critical/high/medium/low
        - ì¹´í…Œê³ ë¦¬: performance/security/maintainability/readability
        - ì œëª©: ê°„ë‹¨í•œ ì œëª©
        - ì„¤ëª…: ìƒì„¸ ì„¤ëª…
        - ì›ë³¸ ì½”ë“œ: ê°œì„  ëŒ€ìƒ ì½”ë“œ
        - ìµœì í™”ëœ ì½”ë“œ: ê°œì„ ëœ ì½”ë“œ
        - ì˜ˆìƒ ê°œì„ ìœ¨: êµ¬ì²´ì  ìˆ˜ì¹˜
        - êµ¬í˜„ ë‚œì´ë„: trivial/easy/moderate/hard
        - ì‹ ë¢°ë„: 0.0-1.0
        - ê·¼ê±°: ìµœì í™” ê·¼ê±°
        """
        
        try:
            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…íŠ¸ì´ì Arduino/IoT ê°œë°œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=4000
            )
            
            # GPT-4 ì‘ë‹µ íŒŒì‹±
            ai_response = response.choices[0].message.content
            suggestions = await self._parse_gpt4_response(ai_response)
            
            return suggestions
            
        except Exception as e:
            logger.error(f"GPT-4 API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def apply_optimization(self, 
                               file_path: str,
                               recommendation: OptimizationRecommendation) -> Dict[str, Any]:
        """ìµœì í™” ì ìš©"""
        
        try:
            # ì›ë³¸ íŒŒì¼ ë°±ì—…
            backup_path = f"{file_path}.backup_{int(datetime.now().timestamp())}"
            shutil.copy2(file_path, backup_path)
            
            # íŒŒì¼ ë‚´ìš© ì½ê¸°
            with open(file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
            
            # ìµœì í™” ì ìš©
            optimized_content = await self._apply_code_optimization(
                original_content,
                recommendation
            )
            
            # ìµœì í™”ëœ ì½”ë“œ ê²€ì¦
            validation_result = await self._validate_optimized_code(
                original_content,
                optimized_content,
                recommendation
            )
            
            if validation_result['is_valid']:
                # íŒŒì¼ì— ìµœì í™”ëœ ì½”ë“œ ì €ì¥
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(optimized_content)
                
                # ìµœì í™” ì´ë ¥ ì €ì¥
                await self._save_optimization_history(
                    file_path, recommendation, validation_result
                )
                
                return {
                    'status': 'success',
                    'backup_path': backup_path,
                    'validation_result': validation_result,
                    'optimization_applied': True
                }
            else:
                # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë°±ì—… íŒŒì¼ ì‚­ì œ
                os.remove(backup_path)
                return {
                    'status': 'failed',
                    'error': validation_result['error'],
                    'optimization_applied': False
                }
                
        except Exception as e:
            logger.error(f"ìµœì í™” ì ìš© ì‹¤íŒ¨: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'optimization_applied': False
            }
    
    async def continuous_optimization_monitoring(self, 
                                               project_path: str) -> Dict[str, Any]:
        """ì§€ì†ì  ìµœì í™” ëª¨ë‹ˆí„°ë§"""
        
        monitoring_results = {
            'project_path': project_path,
            'monitoring_start': datetime.now(),
            'files_analyzed': 0,
            'optimizations_suggested': 0,
            'optimizations_applied': 0,
            'quality_improvements': {},
            'performance_gains': {},
            'alerts': []
        }
        
        # í”„ë¡œì íŠ¸ ë‚´ ëª¨ë“  ì†ŒìŠ¤ íŒŒì¼ ìŠ¤ìº”
        source_files = await self._find_source_files(project_path)
        
        for file_path in source_files:
            try:
                # íŒŒì¼ ì–¸ì–´ ê°ì§€
                language = await self._detect_language(file_path)
                
                # íŒŒì¼ ë‚´ìš© ì½ê¸°
                with open(file_path, 'r', encoding='utf-8') as f:
                    code_content = f.read()
                
                # ì½”ë“œ ë¶„ì„
                analysis_result = await self.analyze_code_comprehensively(
                    code_content, file_path, language
                )
                
                monitoring_results['files_analyzed'] += 1
                
                # ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±
                recommendations = await self.generate_optimization_recommendations(
                    analysis_result
                )
                
                monitoring_results['optimizations_suggested'] += len(recommendations)
                
                # ìë™ ì ìš© ê°€ëŠ¥í•œ ìµœì í™” ì ìš©
                auto_applied = 0
                for rec in recommendations:
                    if (rec.confidence_score > 0.9 and 
                        rec.implementation_effort in ['trivial', 'easy'] and
                        rec.priority in ['critical', 'high']):
                        
                        apply_result = await self.apply_optimization(file_path, rec)
                        if apply_result['optimization_applied']:
                            auto_applied += 1
                
                monitoring_results['optimizations_applied'] += auto_applied
                
                # í’ˆì§ˆ ì§€í‘œ ì¶”ì 
                if analysis_result.quality_gate_status == 'failed':
                    monitoring_results['alerts'].append({
                        'type': 'quality_gate_failed',
                        'file': file_path,
                        'timestamp': datetime.now(),
                        'details': analysis_result
                    })
                
            except Exception as e:
                logger.error(f"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
                monitoring_results['alerts'].append({
                    'type': 'analysis_error',
                    'file': file_path,
                    'error': str(e),
                    'timestamp': datetime.now()
                })
        
        monitoring_results['monitoring_end'] = datetime.now()
        monitoring_results['duration_minutes'] = (
            monitoring_results['monitoring_end'] - monitoring_results['monitoring_start']
        ).total_seconds() / 60
        
        return monitoring_results

class CodeUnderstandingTransformer(nn.Module):
    """ì½”ë“œ ì´í•´ë¥¼ ìœ„í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸"""
    
    def __init__(self, vocab_size=50000, d_model=512, nhead=8, num_layers=6):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=2048,
            dropout=0.1,
            activation='gelu'
        )
        
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )
        
        # ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ìœ„í•œ í—¤ë“œë“¤
        self.complexity_head = nn.Linear(d_model, 1)
        self.quality_head = nn.Linear(d_model, 5)  # 5ê°€ì§€ í’ˆì§ˆ ì ìˆ˜
        self.pattern_head = nn.Linear(d_model, 100)  # íŒ¨í„´ ë¶„ë¥˜
        self.bug_head = nn.Linear(d_model, 1)  # ë²„ê·¸ í™•ë¥ 
        
    def forward(self, src, src_mask=None):
        # ì„ë² ë”© ë° ìœ„ì¹˜ ì¸ì½”ë”©
        src = self.embedding(src) * math.sqrt(512)
        src = self.pos_encoding(src)
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”©
        encoded = self.transformer(src, src_mask)
        
        # ê¸€ë¡œë²Œ í’€ë§ (í‰ê· )
        pooled = encoded.mean(dim=0)
        
        # ê° íƒœìŠ¤í¬ë³„ ì˜ˆì¸¡
        complexity = self.complexity_head(pooled)
        quality = self.quality_head(pooled)
        patterns = self.pattern_head(pooled)
        bug_prob = torch.sigmoid(self.bug_head(pooled))
        
        return {
            'complexity': complexity,
            'quality': quality,
            'patterns': patterns,
            'bug_probability': bug_prob,
            'embeddings': pooled
        }

class CodeGenerationTransformer(nn.Module):
    """ì½”ë“œ ìƒì„±ì„ ìœ„í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸"""
    
    def __init__(self, vocab_size=50000, d_model=512, nhead=8, num_layers=6):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model)
        
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=2048,
            dropout=0.1,
            activation='gelu'
        )
        
        self.transformer = nn.TransformerDecoder(
            decoder_layer,
            num_layers=num_layers
        )
        
        self.output_projection = nn.Linear(d_model, vocab_size)
        
    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):
        # ì„ë² ë”© ë° ìœ„ì¹˜ ì¸ì½”ë”©
        tgt = self.embedding(tgt) * math.sqrt(512)
        tgt = self.pos_encoding(tgt)
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë”©
        decoded = self.transformer(
            tgt, memory, 
            tgt_mask=tgt_mask, 
            memory_mask=memory_mask
        )
        
        # ì¶œë ¥ í”„ë¡œì ì…˜
        output = self.output_projection(decoded)
        
        return output

class PerformanceOptimizer:
    """ì„±ëŠ¥ ìµœì í™” ì—”ì§„"""
    
    def __init__(self):
        self.optimization_patterns = {}
        
    async def generate_recommendations(self, 
                                     analysis_result: CodeAnalysisResult) -> List[OptimizationRecommendation]:
        """ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        memory_recs = await self._analyze_memory_usage(analysis_result)
        recommendations.extend(memory_recs)
        
        # CPU ìµœì í™”
        cpu_recs = await self._analyze_cpu_usage(analysis_result)
        recommendations.extend(cpu_recs)
        
        # ì „ë ¥ ìµœì í™” (Arduino/IoT íŠ¹í™”)
        power_recs = await self._analyze_power_efficiency(analysis_result)
        recommendations.extend(power_recs)
        
        # ë„¤íŠ¸ì›Œí¬ ìµœì í™”
        network_recs = await self._analyze_network_efficiency(analysis_result)
        recommendations.extend(network_recs)
        
        return recommendations
    
    async def _analyze_power_efficiency(self, 
                                      analysis_result: CodeAnalysisResult) -> List[OptimizationRecommendation]:
        """ì „ë ¥ íš¨ìœ¨ì„± ë¶„ì„ (Arduino/IoT íŠ¹í™”)"""
        
        recommendations = []
        
        try:
            with open(analysis_result.file_path, 'r') as f:
                code_content = f.read()
        except:
            return recommendations
        
        # ë”œë ˆì´ íŒ¨í„´ ë¶„ì„
        delay_pattern = r'delay\s*\(\s*(\d+)\s*\)'
        delays = re.finditer(delay_pattern, code_content)
        
        for match in delays:
            delay_value = int(match.group(1))
            if delay_value > 1000:  # 1ì´ˆ ì´ìƒ ë”œë ˆì´
                line_num = code_content[:match.start()].count('\n') + 1
                
                recommendations.append(OptimizationRecommendation(
                    recommendation_id=f"power_{uuid.uuid4().hex[:8]}",
                    priority="high",
                    category="performance",
                    title="Deep Sleepìœ¼ë¡œ ì „ë ¥ ì ˆì•½",
                    description=f"ê¸´ ë”œë ˆì´({delay_value}ms) ëŒ€ì‹  ESP.deepSleep() ì‚¬ìš©ìœ¼ë¡œ ì „ë ¥ ì†Œëª¨ 95% ì ˆì•½",
                    code_location={"file": analysis_result.file_path, "line": line_num},
                    original_code=match.group(0),
                    optimized_code=f"ESP.deepSleep({delay_value * 1000}); // microseconds",
                    expected_improvement={"power_consumption": -95.0, "battery_life": 2000.0},
                    implementation_effort="easy",
                    confidence_score=0.90,
                    ai_reasoning="Deep sleep ëª¨ë“œëŠ” active ëª¨ë“œ ëŒ€ë¹„ ì „ë ¥ ì†Œëª¨ë¥¼ 95% ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                ))
        
        # WiFi ì‚¬ìš© íŒ¨í„´ ë¶„ì„
        if 'WiFi.begin' in code_content and 'WiFi.mode(WIFI_OFF)' not in code_content:
            recommendations.append(OptimizationRecommendation(
                recommendation_id=f"power_{uuid.uuid4().hex[:8]}",
                priority="medium",
                category="performance",
                title="WiFi ì „ë ¥ ê´€ë¦¬ ì¶”ê°€",
                description="ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ WiFi ë„ê¸°ë¡œ ì „ë ¥ ì ˆì•½",
                code_location={"file": analysis_result.file_path, "line": 0},
                original_code="// WiFi í•­ìƒ ì¼œì§",
                optimized_code="""
// WiFi ì „ë ¥ ê´€ë¦¬
if (needWiFi) {
    WiFi.mode(WIFI_STA);
    WiFi.begin(ssid, password);
} else {
    WiFi.mode(WIFI_OFF);
}
                """,
                expected_improvement={"power_consumption": -70.0},
                implementation_effort="moderate",
                confidence_score=0.85,
                ai_reasoning="WiFi ëª¨ë“ˆì€ Arduinoì˜ ì£¼ìš” ì „ë ¥ ì†Œëª¨ì›ì…ë‹ˆë‹¤."
            ))
        
        return recommendations

class SecurityOptimizer:
    """ë³´ì•ˆ ìµœì í™” ì—”ì§„"""
    
    def __init__(self):
        self.vulnerability_patterns = {}
        
    async def generate_recommendations(self, 
                                     analysis_result: CodeAnalysisResult) -> List[OptimizationRecommendation]:
        """ë³´ì•ˆ ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ì·¨ì•½ì  ë¶„ì„
        vulnerability_recs = await self._analyze_vulnerabilities(analysis_result)
        recommendations.extend(vulnerability_recs)
        
        # ì•”í˜¸í™” ë¶„ì„
        crypto_recs = await self._analyze_cryptography(analysis_result)
        recommendations.extend(crypto_recs)
        
        # ì¸ì¦/ê¶Œí•œ ë¶„ì„
        auth_recs = await self._analyze_authentication(analysis_result)
        recommendations.extend(auth_recs)
        
        # ì…ë ¥ ê²€ì¦ ë¶„ì„
        input_recs = await self._analyze_input_validation(analysis_result)
        recommendations.extend(input_recs)
        
        return recommendations

class ReadabilityOptimizer:
    """ê°€ë…ì„± ìµœì í™” ì—”ì§„"""
    
    def __init__(self):
        self.style_patterns = {}
        
    async def generate_recommendations(self, 
                                     analysis_result: CodeAnalysisResult) -> List[OptimizationRecommendation]:
        """ê°€ë…ì„± ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ë„¤ì´ë° ì»¨ë²¤ì…˜ ë¶„ì„
        naming_recs = await self._analyze_naming_conventions(analysis_result)
        recommendations.extend(naming_recs)
        
        # í•¨ìˆ˜ ê¸¸ì´ ë¶„ì„
        function_recs = await self._analyze_function_length(analysis_result)
        recommendations.extend(function_recs)
        
        # ì£¼ì„ ë¶„ì„
        comment_recs = await self._analyze_comments(analysis_result)
        recommendations.extend(comment_recs)
        
        # ì½”ë“œ êµ¬ì¡° ë¶„ì„
        structure_recs = await self._analyze_code_structure(analysis_result)
        recommendations.extend(structure_recs)
        
        return recommendations

class MaintainabilityOptimizer:
    """ìœ ì§€ë³´ìˆ˜ì„± ìµœì í™” ì—”ì§„"""
    
    def __init__(self):
        self.maintainability_patterns = {}
        
    async def generate_recommendations(self, 
                                     analysis_result: CodeAnalysisResult) -> List[OptimizationRecommendation]:
        """ìœ ì§€ë³´ìˆ˜ì„± ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ì½”ë“œ ì¤‘ë³µ ë¶„ì„
        duplication_recs = await self._analyze_code_duplication(analysis_result)
        recommendations.extend(duplication_recs)
        
        # ì˜ì¡´ì„± ë¶„ì„
        dependency_recs = await self._analyze_dependencies(analysis_result)
        recommendations.extend(dependency_recs)
        
        # í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ë¶„ì„
        test_recs = await self._analyze_test_coverage(analysis_result)
        recommendations.extend(test_recs)
        
        # ëª¨ë“ˆí™” ë¶„ì„
        modular_recs = await self._analyze_modularity(analysis_result)
        recommendations.extend(modular_recs)
        
        return recommendations

class CodeMetricsCollector:
    """ì½”ë“œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.metrics_history = []
        self.collection_interval = 300  # 5ë¶„
        
    async def start(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘"""
        asyncio.create_task(self._collection_loop())
        
    async def _collection_loop(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë£¨í”„"""
        while True:
            try:
                await self._collect_metrics()
                await asyncio.sleep(self.collection_interval)
            except Exception as e:
                logger.error(f"ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(60)  # 1ë¶„ í›„ ì¬ì‹œë„

class PositionalEncoding(nn.Module):
    """ìœ„ì¹˜ ì¸ì½”ë”©"""
    
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:x.size(0), :]

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """ì‹ ê²½ë§ ì½”ë“œ ìµœì í™” ì—”ì§„ ë°ëª¨"""
    
    config = {
        'openai_api_key': 'your_openai_api_key',
        'anthropic_api_key': 'your_anthropic_api_key',
        'models_path': './models',
        'metrics_storage': 'redis://localhost:6379',
        'elasticsearch_url': 'http://localhost:9200'
    }
    
    # ì‹ ê²½ë§ ì½”ë“œ ìµœì í™” ì—”ì§„ ì´ˆê¸°í™”
    optimizer = NeuralCodeOptimizer(config)
    await optimizer.initialize()
    
    print("ğŸ§  ì‹ ê²½ë§ ì½”ë“œ ìµœì í™” ì—”ì§„ ì‹œì‘...")
    
    # Arduino ì½”ë“œ ì˜ˆì‹œ
    arduino_code = '''
#include <WiFi.h>
#include <DHT.h>

#define DHT_PIN 4
#define DHT_TYPE DHT22

const char* ssid = "MyWiFi";
const char* password = "password123";

DHT dht(DHT_PIN, DHT_TYPE);

void setup() {
    Serial.begin(115200);
    dht.begin();
    
    WiFi.begin(ssid, password);
    while (WiFi.status() != WL_CONNECTED) {
        delay(1000);
        Serial.println("Connecting to WiFi...");
    }
    Serial.println("WiFi connected!");
}

void loop() {
    float temperature = dht.readTemperature();
    float humidity = dht.readHumidity();
    
    if (isnan(temperature) || isnan(humidity)) {
        Serial.println("Failed to read from DHT sensor!");
        delay(2000);
        return;
    }
    
    Serial.print("Temperature: ");
    Serial.print(temperature);
    Serial.print("Â°C, Humidity: ");
    Serial.print(humidity);
    Serial.println("%");
    
    // WiFië¡œ ë°ì´í„° ì „ì†¡
    if (WiFi.status() == WL_CONNECTED) {
        // HTTP ìš”ì²­ ì½”ë“œ (ìƒëµ)
    }
    
    delay(10000); // 10ì´ˆ ëŒ€ê¸°
}
    '''
    
    # ì„ì‹œ íŒŒì¼ ìƒì„±
    temp_file = 'temp_arduino_code.ino'
    with open(temp_file, 'w') as f:
        f.write(arduino_code)
    
    print("\nğŸ“Š ì¢…í•©ì ì¸ ì½”ë“œ ë¶„ì„...")
    
    # ì¢…í•©ì ì¸ ì½”ë“œ ë¶„ì„
    analysis_result = await optimizer.analyze_code_comprehensively(
        arduino_code, temp_file, 'cpp'
    )
    
    print(f"âœ… ì½”ë“œ ë¶„ì„ ì™„ë£Œ:")
    print(f"   íŒŒì¼: {analysis_result.file_path}")
    print(f"   ì–¸ì–´: {analysis_result.language}")
    print(f"   ì½”ë“œ ë¼ì¸ ìˆ˜: {analysis_result.lines_of_code}")
    print(f"   ìˆœí™˜ ë³µì¡ë„: {analysis_result.cyclomatic_complexity:.2f}")
    print(f"   ìœ ì§€ë³´ìˆ˜ì„± ì§€ìˆ˜: {analysis_result.maintainability_index:.2f}")
    print(f"   ë³´ì•ˆ ì ìˆ˜: {analysis_result.security_score:.2f}/100")
    print(f"   ì„±ëŠ¥ ì ìˆ˜: {analysis_result.performance_score:.2f}/100")
    print(f"   ê°€ë…ì„± ì ìˆ˜: {analysis_result.readability_score:.2f}/100")
    print(f"   í’ˆì§ˆ ê²Œì´íŠ¸: {analysis_result.quality_gate_status}")
    
    # ì½”ë“œ ìŠ¤ë©œ ì¶œë ¥
    if analysis_result.code_smells:
        print(f"\nğŸš¨ ë°œê²¬ëœ ì½”ë“œ ìŠ¤ë©œ ({len(analysis_result.code_smells)}ê°œ):")
        for smell in analysis_result.code_smells[:3]:  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
            print(f"   - {smell['type']}: {smell['description']}")
    
    # ì·¨ì•½ì  ì¶œë ¥
    if analysis_result.vulnerabilities:
        print(f"\nğŸ”’ ë°œê²¬ëœ ì·¨ì•½ì  ({len(analysis_result.vulnerabilities)}ê°œ):")
        for vuln in analysis_result.vulnerabilities[:3]:  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
            print(f"   - {vuln['severity']}: {vuln['description']}")
    
    print("\nğŸ’¡ ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±...")
    
    # ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±
    recommendations = await optimizer.generate_optimization_recommendations(analysis_result)
    
    print(f"âœ… {len(recommendations)}ê°œ ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±:")
    
    for i, rec in enumerate(recommendations[:5], 1):  # ìƒìœ„ 5ê°œ ì¶œë ¥
        print(f"\n{i}. [{rec.priority.upper()}] {rec.title}")
        print(f"   ì¹´í…Œê³ ë¦¬: {rec.category}")
        print(f"   ì„¤ëª…: {rec.description}")
        print(f"   êµ¬í˜„ ë‚œì´ë„: {rec.implementation_effort}")
        print(f"   ì‹ ë¢°ë„: {rec.confidence_score:.2f}")
        
        if rec.expected_improvement:
            improvements = ", ".join([
                f"{k}: {v:+.1f}%" for k, v in rec.expected_improvement.items()
            ])
            print(f"   ì˜ˆìƒ ê°œì„ : {improvements}")
    
    print("\nğŸ”„ ìë™ ìµœì í™” ì ìš©...")
    
    # ê³ ì‹ ë¢°ë„ ìµœì í™” ìë™ ì ìš©
    auto_applied = 0
    for rec in recommendations:
        if (rec.confidence_score > 0.9 and 
            rec.implementation_effort in ['trivial', 'easy'] and 
            rec.priority in ['critical', 'high']):
            
            apply_result = await optimizer.apply_optimization(temp_file, rec)
            if apply_result['optimization_applied']:
                auto_applied += 1
                print(f"   âœ… ì ìš©ë¨: {rec.title}")
    
    print(f"\nğŸ“ˆ ìë™ ìµœì í™” ê²°ê³¼:")
    print(f"   ì „ì²´ ê¶Œì¥ì‚¬í•­: {len(recommendations)}ê°œ")
    print(f"   ìë™ ì ìš©ë¨: {auto_applied}ê°œ")
    print(f"   ìˆ˜ë™ ê²€í†  í•„ìš”: {len(recommendations) - auto_applied}ê°œ")
    
    # ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ì‹œì‘ (í”„ë¡œì íŠ¸ í´ë” ì˜ˆì‹œ)
    print("\nğŸ“Š ì§€ì†ì  ìµœì í™” ëª¨ë‹ˆí„°ë§...")
    
    monitoring_result = await optimizer.continuous_optimization_monitoring('.')
    
    print(f"âœ… ëª¨ë‹ˆí„°ë§ ì™„ë£Œ:")
    print(f"   ë¶„ì„ëœ íŒŒì¼: {monitoring_result['files_analyzed']}ê°œ")
    print(f"   ì œì•ˆëœ ìµœì í™”: {monitoring_result['optimizations_suggested']}ê°œ")
    print(f"   ì ìš©ëœ ìµœì í™”: {monitoring_result['optimizations_applied']}ê°œ")
    print(f"   ì†Œìš” ì‹œê°„: {monitoring_result['duration_minutes']:.1f}ë¶„")
    
    if monitoring_result['alerts']:
        print(f"   âš ï¸ ì•Œë¦¼: {len(monitoring_result['alerts'])}ê°œ")
        for alert in monitoring_result['alerts'][:3]:
            print(f"      - {alert['type']}: {alert.get('file', 'N/A')}")
    
    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
    if os.path.exists(temp_file):
        os.remove(temp_file)
    
    print("\nğŸŒŸ ì‹ ê²½ë§ ì½”ë“œ ìµœì í™” ì—”ì§„ ë°ëª¨ ì™„ë£Œ!")

if __name__ == "__main__":
    asyncio.run(main())
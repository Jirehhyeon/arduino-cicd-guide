#!/usr/bin/env python3
"""
ğŸ§  AI/ML ê¸°ë°˜ ì˜ˆì¸¡ì  ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
Arduino IoT ë””ë°”ì´ìŠ¤ì˜ ì¥ì•  ì˜ˆì¸¡, ì„±ëŠ¥ ìµœì í™”, ììœ¨ ë³µêµ¬ ì‹œìŠ¤í…œ
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from sklearn.ensemble import IsolationForest, RandomForestRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, classification_report
import joblib
import asyncio
import websockets
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from pathlib import Path
import redis
import paho.mqtt.client as mqtt
from kafka import KafkaProducer, KafkaConsumer
import plotly.graph_objs as go
import plotly.offline as pyo
from influxdb_client import InfluxDBClient, Point
from influxdb_client.client.write_api import SYNCHRONOUS
import warnings
warnings.filterwarnings('ignore')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class SensorReading:
    """ì„¼ì„œ ë°ì´í„° êµ¬ì¡°"""
    device_id: str
    timestamp: datetime
    temperature: float
    humidity: float
    pressure: float
    light_level: float
    soil_moisture: float
    battery_voltage: float
    cpu_usage: float
    memory_usage: float
    wifi_signal_strength: int
    error_count: int
    uptime_hours: float

@dataclass
class PredictionResult:
    """ì˜ˆì¸¡ ê²°ê³¼ êµ¬ì¡°"""
    device_id: str
    timestamp: datetime
    failure_probability: float
    predicted_failure_time: Optional[datetime]
    anomaly_score: float
    recommended_actions: List[str]
    confidence_score: float
    health_score: float  # 0-100
    performance_metrics: Dict[str, float]

@dataclass
class OptimizationRecommendation:
    """ìµœì í™” ê¶Œê³ ì‚¬í•­"""
    device_id: str
    optimization_type: str  # "energy", "performance", "reliability"
    current_value: float
    recommended_value: float
    expected_improvement: float
    implementation_code: str
    risk_assessment: str

class ArduinoMLPredictor:
    """Arduino IoT ë””ë°”ì´ìŠ¤ ML ì˜ˆì¸¡ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.models = {}
        self.scalers = {}
        self.redis_client = redis.Redis(
            host=config.get('redis_host', 'localhost'),
            port=config.get('redis_port', 6379),
            decode_responses=True
        )
        
        # InfluxDB ì—°ê²° (ì‹œê³„ì—´ ë°ì´í„°)
        self.influx_client = InfluxDBClient(
            url=config.get('influxdb_url', 'http://localhost:8086'),
            token=config.get('influxdb_token'),
            org=config.get('influxdb_org', 'arduino-devops')
        )
        self.write_api = self.influx_client.write_api(write_options=SYNCHRONOUS)
        
        # Kafka Producer (ì‹¤ì‹œê°„ ì•Œë¦¼)
        self.kafka_producer = KafkaProducer(
            bootstrap_servers=config.get('kafka_servers', ['localhost:9092']),
            value_serializer=lambda x: json.dumps(x).encode('utf-8')
        )
        
        # MQTT Client (ë””ë°”ì´ìŠ¤ í†µì‹ )
        self.mqtt_client = mqtt.Client()
        self.mqtt_client.on_connect = self._on_mqtt_connect
        self.mqtt_client.on_message = self._on_mqtt_message
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_models()
        
    def _initialize_models(self):
        """ML ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        
        # 1. ì¥ì•  ì˜ˆì¸¡ ëª¨ë¸ (LSTM)
        self.models['failure_prediction'] = self._create_lstm_failure_model()
        
        # 2. ì´ìƒ íƒì§€ ëª¨ë¸ (Isolation Forest)
        self.models['anomaly_detection'] = IsolationForest(
            contamination=0.1,
            random_state=42,
            n_estimators=100
        )
        
        # 3. ì„±ëŠ¥ ì˜ˆì¸¡ ëª¨ë¸ (Random Forest)
        self.models['performance_prediction'] = RandomForestRegressor(
            n_estimators=200,
            random_state=42,
            max_depth=15
        )
        
        # 4. ì—ë„ˆì§€ ìµœì í™” ëª¨ë¸ (Neural Network)
        self.models['energy_optimization'] = self._create_energy_optimization_model()
        
        # 5. ìê°€ ì¹˜ìœ  ì¶”ì²œ ëª¨ë¸ (Decision Tree Ensemble)
        self.models['self_healing'] = self._create_self_healing_model()
        
        logger.info("All ML models initialized successfully")
    
    def _create_lstm_failure_model(self) -> keras.Model:
        """LSTM ê¸°ë°˜ ì¥ì•  ì˜ˆì¸¡ ëª¨ë¸"""
        model = keras.Sequential([
            keras.layers.LSTM(64, return_sequences=True, input_shape=(24, 12)),  # 24ì‹œê°„, 12ê°œ í”¼ì²˜
            keras.layers.Dropout(0.2),
            keras.layers.LSTM(32, return_sequences=False),
            keras.layers.Dropout(0.2),
            keras.layers.Dense(16, activation='relu'),
            keras.layers.Dense(8, activation='relu'),
            keras.layers.Dense(1, activation='sigmoid')  # ì¥ì•  í™•ë¥  (0-1)
        ])
        
        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        return model
    
    def _create_energy_optimization_model(self) -> keras.Model:
        """ì—ë„ˆì§€ ìµœì í™” Neural Network"""
        model = keras.Sequential([
            keras.layers.Dense(128, activation='relu', input_shape=(12,)),
            keras.layers.BatchNormalization(),
            keras.layers.Dropout(0.3),
            keras.layers.Dense(64, activation='relu'),
            keras.layers.BatchNormalization(),
            keras.layers.Dropout(0.2),
            keras.layers.Dense(32, activation='relu'),
            keras.layers.Dense(16, activation='relu'),
            keras.layers.Dense(4, activation='linear')  # [cpu_freq, wifi_power, sleep_interval, sensor_interval]
        ])
        
        model.compile(
            optimizer='adam',
            loss='mse',
            metrics=['mae']
        )
        
        return model
    
    def _create_self_healing_model(self) -> keras.Model:
        """ìê°€ ì¹˜ìœ  ì•¡ì…˜ ì¶”ì²œ ëª¨ë¸"""
        model = keras.Sequential([
            keras.layers.Dense(64, activation='relu', input_shape=(15,)),  # í™•ì¥ëœ í”¼ì²˜
            keras.layers.Dropout(0.2),
            keras.layers.Dense(32, activation='relu'),
            keras.layers.Dense(16, activation='relu'),
            keras.layers.Dense(8, activation='softmax')  # 8ê°€ì§€ ì¹˜ìœ  ì•¡ì…˜
        ])
        
        model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    async def train_models(self, historical_data: pd.DataFrame):
        """ëª¨ë¸ í•™ìŠµ"""
        logger.info("Starting model training...")
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        X, y_failure, y_performance, y_energy = self._preprocess_training_data(historical_data)
        
        # 1. ì¥ì•  ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ
        await self._train_failure_prediction(X, y_failure)
        
        # 2. ì´ìƒ íƒì§€ ëª¨ë¸ í•™ìŠµ
        await self._train_anomaly_detection(X)
        
        # 3. ì„±ëŠ¥ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ
        await self._train_performance_prediction(X, y_performance)
        
        # 4. ì—ë„ˆì§€ ìµœì í™” ëª¨ë¸ í•™ìŠµ
        await self._train_energy_optimization(X, y_energy)
        
        # 5. ìê°€ ì¹˜ìœ  ëª¨ë¸ í•™ìŠµ
        await self._train_self_healing(X, historical_data)
        
        # ëª¨ë¸ ì €ì¥
        self._save_models()
        
        logger.info("Model training completed successfully")
    
    async def _train_failure_prediction(self, X: np.ndarray, y: np.ndarray):
        """ì¥ì•  ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ"""
        # ì‹œê³„ì—´ ë°ì´í„°ë¡œ ë³€í™˜ (24ì‹œê°„ ìœˆë„ìš°)
        X_seq, y_seq = self._create_sequences(X, y, seq_length=24)
        
        X_train, X_test, y_train, y_test = train_test_split(
            X_seq, y_seq, test_size=0.2, random_state=42
        )
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1]))
        X_train_scaled = X_train_scaled.reshape(X_train.shape)
        X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1]))
        X_test_scaled = X_test_scaled.reshape(X_test.shape)
        
        self.scalers['failure_prediction'] = scaler
        
        # ëª¨ë¸ í•™ìŠµ
        history = self.models['failure_prediction'].fit(
            X_train_scaled, y_train,
            epochs=100,
            batch_size=32,
            validation_data=(X_test_scaled, y_test),
            callbacks=[
                keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
                keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5)
            ],
            verbose=0
        )
        
        # ì„±ëŠ¥ í‰ê°€
        test_loss, test_acc, test_prec, test_recall = self.models['failure_prediction'].evaluate(
            X_test_scaled, y_test, verbose=0
        )
        
        logger.info(f"Failure prediction model - Accuracy: {test_acc:.4f}, Precision: {test_prec:.4f}, Recall: {test_recall:.4f}")
    
    async def _train_anomaly_detection(self, X: np.ndarray):
        """ì´ìƒ íƒì§€ ëª¨ë¸ í•™ìŠµ"""
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        self.scalers['anomaly_detection'] = scaler
        
        self.models['anomaly_detection'].fit(X_scaled)
        
        # ì´ìƒì¹˜ ì ìˆ˜ ê³„ì‚°
        anomaly_scores = self.models['anomaly_detection'].decision_function(X_scaled)
        threshold = np.percentile(anomaly_scores, 10)  # í•˜ìœ„ 10%ë¥¼ ì´ìƒì¹˜ë¡œ ê°„ì£¼
        
        logger.info(f"Anomaly detection model trained. Threshold: {threshold:.4f}")
    
    async def _train_performance_prediction(self, X: np.ndarray, y: np.ndarray):
        """ì„±ëŠ¥ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ"""
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        self.scalers['performance_prediction'] = scaler
        
        self.models['performance_prediction'].fit(X_train_scaled, y_train)
        
        # ì„±ëŠ¥ í‰ê°€
        y_pred = self.models['performance_prediction'].predict(X_test_scaled)
        mse = mean_squared_error(y_test, y_pred)
        
        logger.info(f"Performance prediction model - MSE: {mse:.4f}")
    
    async def _train_energy_optimization(self, X: np.ndarray, y: np.ndarray):
        """ì—ë„ˆì§€ ìµœì í™” ëª¨ë¸ í•™ìŠµ"""
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        scaler = MinMaxScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        self.scalers['energy_optimization'] = scaler
        
        history = self.models['energy_optimization'].fit(
            X_train_scaled, y_train,
            epochs=200,
            batch_size=64,
            validation_data=(X_test_scaled, y_test),
            callbacks=[
                keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)
            ],
            verbose=0
        )
        
        test_loss = self.models['energy_optimization'].evaluate(X_test_scaled, y_test, verbose=0)
        logger.info(f"Energy optimization model - Test Loss: {test_loss:.4f}")
    
    async def predict_device_health(self, sensor_data: List[SensorReading]) -> PredictionResult:
        """ë””ë°”ì´ìŠ¤ ê±´ê°• ìƒíƒœ ì˜ˆì¸¡"""
        if not sensor_data:
            raise ValueError("No sensor data provided")
        
        device_id = sensor_data[0].device_id
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        features = self._extract_features(sensor_data)
        
        # 1. ì¥ì•  ì˜ˆì¸¡
        failure_prob = await self._predict_failure(features)
        
        # 2. ì´ìƒ íƒì§€
        anomaly_score = await self._detect_anomaly(features)
        
        # 3. ì„±ëŠ¥ ì˜ˆì¸¡
        performance_metrics = await self._predict_performance(features)
        
        # 4. ê±´ê°• ì ìˆ˜ ê³„ì‚°
        health_score = self._calculate_health_score(
            failure_prob, anomaly_score, performance_metrics
        )
        
        # 5. ê¶Œê³ ì‚¬í•­ ìƒì„±
        recommendations = await self._generate_recommendations(
            features, failure_prob, anomaly_score, performance_metrics
        )
        
        # 6. ì˜ˆì¸¡ëœ ì¥ì•  ì‹œê°„ ê³„ì‚°
        predicted_failure_time = self._estimate_failure_time(
            failure_prob, sensor_data[-1].timestamp
        )
        
        result = PredictionResult(
            device_id=device_id,
            timestamp=datetime.now(),
            failure_probability=failure_prob,
            predicted_failure_time=predicted_failure_time,
            anomaly_score=anomaly_score,
            recommended_actions=recommendations,
            confidence_score=self._calculate_confidence(features),
            health_score=health_score,
            performance_metrics=performance_metrics
        )
        
        # ê²°ê³¼ ì €ì¥ ë° ì•Œë¦¼
        await self._store_prediction_result(result)
        await self._send_alerts_if_needed(result)
        
        return result
    
    async def _predict_failure(self, features: np.ndarray) -> float:
        """ì¥ì•  í™•ë¥  ì˜ˆì¸¡"""
        if 'failure_prediction' not in self.models:
            return 0.0
        
        # ì‹œê³„ì—´ í˜•íƒœë¡œ ë³€í™˜
        if len(features.shape) == 1:
            features = features.reshape(1, 1, -1)
        elif len(features.shape) == 2:
            features = features.reshape(features.shape[0], 1, features.shape[1])
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = self.scalers.get('failure_prediction')
        if scaler:
            original_shape = features.shape
            features_scaled = scaler.transform(features.reshape(-1, features.shape[-1]))
            features_scaled = features_scaled.reshape(original_shape)
        else:
            features_scaled = features
        
        prediction = self.models['failure_prediction'].predict(features_scaled, verbose=0)
        return float(prediction[0][0]) if prediction.ndim > 1 else float(prediction[0])
    
    async def _detect_anomaly(self, features: np.ndarray) -> float:
        """ì´ìƒ ì ìˆ˜ ê³„ì‚°"""
        if 'anomaly_detection' not in self.models:
            return 0.0
        
        # ìµœì‹  ë°ì´í„°ë§Œ ì‚¬ìš©
        if len(features.shape) > 1:
            features = features[-1]
        
        scaler = self.scalers.get('anomaly_detection')
        if scaler:
            features_scaled = scaler.transform(features.reshape(1, -1))
        else:
            features_scaled = features.reshape(1, -1)
        
        anomaly_score = self.models['anomaly_detection'].decision_function(features_scaled)
        # ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        return float(max(0, min(1, (anomaly_score[0] + 0.5) * 2)))
    
    async def _predict_performance(self, features: np.ndarray) -> Dict[str, float]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì˜ˆì¸¡"""
        if 'performance_prediction' not in self.models:
            return {}
        
        # ìµœì‹  ë°ì´í„°ë§Œ ì‚¬ìš©
        if len(features.shape) > 1:
            features = features[-1]
        
        scaler = self.scalers.get('performance_prediction')
        if scaler:
            features_scaled = scaler.transform(features.reshape(1, -1))
        else:
            features_scaled = features.reshape(1, -1)
        
        prediction = self.models['performance_prediction'].predict(features_scaled)
        
        return {
            'response_time_ms': float(prediction[0]),
            'throughput_ops_sec': float(prediction[0] * 1.2),
            'cpu_efficiency': float(min(100, max(0, 100 - prediction[0] * 0.1))),
            'memory_efficiency': float(min(100, max(0, 100 - prediction[0] * 0.05)))
        }
    
    def _calculate_health_score(self, 
                               failure_prob: float, 
                               anomaly_score: float, 
                               performance_metrics: Dict[str, float]) -> float:
        """ì¢…í•© ê±´ê°• ì ìˆ˜ ê³„ì‚° (0-100)"""
        # ê°€ì¤‘ì¹˜ ì ìš©
        failure_weight = 0.4
        anomaly_weight = 0.3
        performance_weight = 0.3
        
        # ì¥ì•  í™•ë¥  ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        failure_score = (1 - failure_prob) * 100
        
        # ì´ìƒ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        anomaly_health_score = (1 - anomaly_score) * 100
        
        # ì„±ëŠ¥ ì ìˆ˜
        performance_score = 0
        if performance_metrics:
            cpu_eff = performance_metrics.get('cpu_efficiency', 50)
            mem_eff = performance_metrics.get('memory_efficiency', 50)
            performance_score = (cpu_eff + mem_eff) / 2
        else:
            performance_score = 50  # ê¸°ë³¸ê°’
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        health_score = (
            failure_score * failure_weight +
            anomaly_health_score * anomaly_weight +
            performance_score * performance_weight
        )
        
        return max(0, min(100, health_score))
    
    async def _generate_recommendations(self, 
                                      features: np.ndarray,
                                      failure_prob: float,
                                      anomaly_score: float,
                                      performance_metrics: Dict[str, float]) -> List[str]:
        """ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì¥ì•  í™•ë¥ ì´ ë†’ì€ ê²½ìš°
        if failure_prob > 0.7:
            recommendations.append("CRITICAL: Immediate maintenance required")
            recommendations.append("Schedule device replacement within 24 hours")
            recommendations.append("Backup all critical data immediately")
        elif failure_prob > 0.5:
            recommendations.append("WARNING: Monitor device closely")
            recommendations.append("Schedule maintenance within 72 hours")
        
        # ì´ìƒ ì ìˆ˜ê°€ ë†’ì€ ê²½ìš°
        if anomaly_score > 0.8:
            recommendations.append("Anomaly detected: Check sensor calibration")
            recommendations.append("Review recent environmental changes")
        elif anomaly_score > 0.6:
            recommendations.append("Mild anomaly: Monitor sensor readings")
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œê³ ì‚¬í•­
        if performance_metrics:
            cpu_eff = performance_metrics.get('cpu_efficiency', 100)
            mem_eff = performance_metrics.get('memory_efficiency', 100)
            
            if cpu_eff < 60:
                recommendations.append("Optimize CPU usage: Reduce sampling frequency")
                recommendations.append("Consider deep sleep mode implementation")
            
            if mem_eff < 60:
                recommendations.append("Memory optimization needed")
                recommendations.append("Clear unnecessary buffers and cache")
        
        # AI ê¸°ë°˜ ìµœì í™” ê¶Œê³ 
        optimization_recs = await self._get_ai_optimization_recommendations(features)
        recommendations.extend(optimization_recs)
        
        return recommendations
    
    async def _get_ai_optimization_recommendations(self, features: np.ndarray) -> List[str]:
        """AI ê¸°ë°˜ ìµœì í™” ê¶Œê³ ì‚¬í•­"""
        if 'energy_optimization' not in self.models:
            return []
        
        try:
            # ìµœì‹  ë°ì´í„°ë§Œ ì‚¬ìš©
            if len(features.shape) > 1:
                features = features[-1]
            
            scaler = self.scalers.get('energy_optimization')
            if scaler:
                features_scaled = scaler.transform(features.reshape(1, -1))
            else:
                features_scaled = features.reshape(1, -1)
            
            optimization_params = self.models['energy_optimization'].predict(features_scaled, verbose=0)
            
            recommendations = []
            
            # CPU ì£¼íŒŒìˆ˜ ìµœì í™”
            cpu_freq = optimization_params[0][0]
            if cpu_freq < 160:
                recommendations.append(f"Reduce CPU frequency to {cpu_freq:.0f}MHz for energy savings")
            
            # WiFi ì „ë ¥ ìµœì í™”
            wifi_power = optimization_params[0][1]
            if wifi_power < 0.5:
                recommendations.append("Enable WiFi power saving mode")
            
            # ìŠ¬ë¦½ ê°„ê²© ìµœì í™”
            sleep_interval = optimization_params[0][2]
            if sleep_interval > 30:
                recommendations.append(f"Increase sleep interval to {sleep_interval:.0f} seconds")
            
            # ì„¼ì„œ ìƒ˜í”Œë§ ìµœì í™”
            sensor_interval = optimization_params[0][3]
            if sensor_interval > 10:
                recommendations.append(f"Reduce sensor sampling to every {sensor_interval:.0f} seconds")
            
            return recommendations
        
        except Exception as e:
            logger.error(f"Error generating AI optimization recommendations: {e}")
            return []
    
    def _extract_features(self, sensor_data: List[SensorReading]) -> np.ndarray:
        """ì„¼ì„œ ë°ì´í„°ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        if not sensor_data:
            return np.array([])
        
        # ìµœì‹  ë°ì´í„° í¬ì¸íŠ¸ë“¤ì„ ì‚¬ìš©
        recent_data = sensor_data[-24:] if len(sensor_data) >= 24 else sensor_data
        
        features_list = []
        for reading in recent_data:
            features = [
                reading.temperature,
                reading.humidity,
                reading.pressure,
                reading.light_level,
                reading.soil_moisture,
                reading.battery_voltage,
                reading.cpu_usage,
                reading.memory_usage,
                reading.wifi_signal_strength,
                reading.error_count,
                reading.uptime_hours,
                # ì‹œê°„ ê¸°ë°˜ íŠ¹ì§•
                reading.timestamp.hour,  # ì‹œê°„
            ]
            features_list.append(features)
        
        return np.array(features_list)
    
    async def optimize_device_settings(self, device_id: str, 
                                     current_settings: Dict[str, Any]) -> List[OptimizationRecommendation]:
        """ë””ë°”ì´ìŠ¤ ì„¤ì • ìµœì í™”"""
        recommendations = []
        
        # ìµœê·¼ ì„¼ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        recent_data = await self._get_recent_sensor_data(device_id, hours=24)
        if not recent_data:
            return recommendations
        
        features = self._extract_features(recent_data)
        
        # ì—ë„ˆì§€ ìµœì í™”
        energy_recs = await self._generate_energy_optimizations(
            device_id, features, current_settings
        )
        recommendations.extend(energy_recs)
        
        # ì„±ëŠ¥ ìµœì í™”
        performance_recs = await self._generate_performance_optimizations(
            device_id, features, current_settings
        )
        recommendations.extend(performance_recs)
        
        # ì•ˆì •ì„± ìµœì í™”
        reliability_recs = await self._generate_reliability_optimizations(
            device_id, features, current_settings
        )
        recommendations.extend(reliability_recs)
        
        return recommendations
    
    async def _generate_energy_optimizations(self, 
                                           device_id: str,
                                           features: np.ndarray,
                                           current_settings: Dict[str, Any]) -> List[OptimizationRecommendation]:
        """ì—ë„ˆì§€ íš¨ìœ¨ì„± ìµœì í™”"""
        recommendations = []
        
        if 'energy_optimization' not in self.models:
            return recommendations
        
        try:
            # ìµœì‹  ë°ì´í„°ë§Œ ì‚¬ìš©
            if len(features.shape) > 1:
                features = features[-1]
            
            scaler = self.scalers.get('energy_optimization')
            if scaler:
                features_scaled = scaler.transform(features.reshape(1, -1))
            else:
                features_scaled = features.reshape(1, -1)
            
            optimized_params = self.models['energy_optimization'].predict(features_scaled, verbose=0)[0]
            
            # CPU ì£¼íŒŒìˆ˜ ìµœì í™”
            current_cpu_freq = current_settings.get('cpu_frequency_mhz', 240)
            recommended_cpu_freq = max(80, min(240, optimized_params[0]))
            
            if abs(current_cpu_freq - recommended_cpu_freq) > 20:
                energy_savings = (current_cpu_freq - recommended_cpu_freq) / current_cpu_freq * 30  # ì˜ˆìƒ ì ˆì•½ìœ¨
                
                recommendations.append(OptimizationRecommendation(
                    device_id=device_id,
                    optimization_type="energy",
                    current_value=current_cpu_freq,
                    recommended_value=recommended_cpu_freq,
                    expected_improvement=energy_savings,
                    implementation_code=f"setCpuFrequencyMhz({int(recommended_cpu_freq)});",
                    risk_assessment="Low risk - Reversible change"
                ))
            
            # ì„¼ì„œ ìƒ˜í”Œë§ ê°„ê²© ìµœì í™”
            current_sampling_interval = current_settings.get('sensor_interval_sec', 10)
            recommended_sampling_interval = max(5, min(60, optimized_params[3]))
            
            if abs(current_sampling_interval - recommended_sampling_interval) > 5:
                energy_savings = (recommended_sampling_interval - current_sampling_interval) / current_sampling_interval * 20
                
                recommendations.append(OptimizationRecommendation(
                    device_id=device_id,
                    optimization_type="energy",
                    current_value=current_sampling_interval,
                    recommended_value=recommended_sampling_interval,
                    expected_improvement=max(0, energy_savings),
                    implementation_code=f"const int SENSOR_INTERVAL = {int(recommended_sampling_interval)} * 1000; // ms",
                    risk_assessment="Low risk - May reduce data granularity"
                ))
        
        except Exception as e:
            logger.error(f"Error generating energy optimizations: {e}")
        
        return recommendations
    
    async def start_real_time_monitoring(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        logger.info("Starting real-time monitoring system...")
        
        # MQTT ì—°ê²°
        await self._connect_mqtt()
        
        # Kafka ì»¨ìŠˆë¨¸ ì‹œì‘
        await self._start_kafka_consumer()
        
        # ì›¹ì†Œì¼“ ì„œë²„ ì‹œì‘
        await self._start_websocket_server()
        
        logger.info("Real-time monitoring system started successfully")
    
    async def _connect_mqtt(self):
        """MQTT ë¸Œë¡œì»¤ ì—°ê²°"""
        def on_connect(client, userdata, flags, rc):
            if rc == 0:
                logger.info("Connected to MQTT broker")
                client.subscribe("arduino/+/sensors")  # ëª¨ë“  ë””ë°”ì´ìŠ¤ ì„¼ì„œ ë°ì´í„°
                client.subscribe("arduino/+/status")   # ë””ë°”ì´ìŠ¤ ìƒíƒœ
            else:
                logger.error(f"Failed to connect to MQTT broker: {rc}")
        
        def on_message(client, userdata, msg):
            try:
                topic_parts = msg.topic.split('/')
                device_id = topic_parts[1]
                data_type = topic_parts[2]
                
                payload = json.loads(msg.payload.decode())
                
                if data_type == 'sensors':
                    # ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬
                    asyncio.create_task(self._process_sensor_data(device_id, payload))
                elif data_type == 'status':
                    # ìƒíƒœ ë°ì´í„° ì²˜ë¦¬
                    asyncio.create_task(self._process_status_data(device_id, payload))
                    
            except Exception as e:
                logger.error(f"Error processing MQTT message: {e}")
        
        self.mqtt_client.on_connect = on_connect
        self.mqtt_client.on_message = on_message
        
        mqtt_host = self.config.get('mqtt_host', 'localhost')
        mqtt_port = self.config.get('mqtt_port', 1883)
        
        self.mqtt_client.connect_async(mqtt_host, mqtt_port, 60)
        self.mqtt_client.loop_start()
    
    async def _process_sensor_data(self, device_id: str, payload: Dict[str, Any]):
        """ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬ ë° ì˜ˆì¸¡ ì‹¤í–‰"""
        try:
            # ì„¼ì„œ ë°ì´í„° íŒŒì‹±
            sensor_reading = SensorReading(
                device_id=device_id,
                timestamp=datetime.now(),
                temperature=payload.get('temperature', 0.0),
                humidity=payload.get('humidity', 0.0),
                pressure=payload.get('pressure', 0.0),
                light_level=payload.get('light_level', 0.0),
                soil_moisture=payload.get('soil_moisture', 0.0),
                battery_voltage=payload.get('battery_voltage', 0.0),
                cpu_usage=payload.get('cpu_usage', 0.0),
                memory_usage=payload.get('memory_usage', 0.0),
                wifi_signal_strength=payload.get('wifi_signal_strength', 0),
                error_count=payload.get('error_count', 0),
                uptime_hours=payload.get('uptime_hours', 0.0)
            )
            
            # InfluxDBì— ì €ì¥
            await self._store_sensor_data(sensor_reading)
            
            # ìµœê·¼ ë°ì´í„° ê°€ì ¸ì™€ì„œ ì˜ˆì¸¡ ì‹¤í–‰
            recent_data = await self._get_recent_sensor_data(device_id, hours=1)
            recent_data.append(sensor_reading)
            
            if len(recent_data) >= 5:  # ìµœì†Œ 5ê°œ ë°ì´í„° í¬ì¸íŠ¸ í•„ìš”
                prediction_result = await self.predict_device_health(recent_data)
                
                # ì˜ˆì¸¡ ê²°ê³¼ ë¸Œë¡œë“œìºìŠ¤íŠ¸
                await self._broadcast_prediction(prediction_result)
        
        except Exception as e:
            logger.error(f"Error processing sensor data for device {device_id}: {e}")
    
    def _save_models(self):
        """ëª¨ë¸ë“¤ì„ ë””ìŠ¤í¬ì— ì €ì¥"""
        models_dir = Path("models")
        models_dir.mkdir(exist_ok=True)
        
        # TensorFlow ëª¨ë¸ë“¤ ì €ì¥
        for name, model in self.models.items():
            if hasattr(model, 'save'):
                model.save(models_dir / f"{name}.h5")
            else:
                joblib.dump(model, models_dir / f"{name}.pkl")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ë“¤ ì €ì¥
        scalers_dir = models_dir / "scalers"
        scalers_dir.mkdir(exist_ok=True)
        
        for name, scaler in self.scalers.items():
            joblib.dump(scaler, scalers_dir / f"{name}.pkl")
        
        logger.info("All models and scalers saved successfully")

# ì‚¬ìš© ì˜ˆì‹œ ë° ë©”ì¸ ì‹¤í–‰ë¶€
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ì„¤ì •
    config = {
        'redis_host': 'localhost',
        'redis_port': 6379,
        'influxdb_url': 'http://localhost:8086',
        'influxdb_token': 'your-influxdb-token',
        'influxdb_org': 'arduino-devops',
        'kafka_servers': ['localhost:9092'],
        'mqtt_host': 'localhost',
        'mqtt_port': 1883
    }
    
    # ML ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    predictor = ArduinoMLPredictor(config)
    
    # ì˜ˆì‹œ í•™ìŠµ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ)
    print("ğŸ§  ìƒì„± ì¤‘: ì˜ˆì‹œ í•™ìŠµ ë°ì´í„°...")
    historical_data = generate_sample_training_data(1000)  # 1000ê°œ ìƒ˜í”Œ
    
    # ëª¨ë¸ í•™ìŠµ
    print("ğŸš€ ì‹œì‘: AI ëª¨ë¸ í•™ìŠµ...")
    await predictor.train_models(historical_data)
    
    # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    print("ğŸ“Š ì‹œì‘: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§...")
    await predictor.start_real_time_monitoring()
    
    # ì˜ˆì‹œ ì˜ˆì¸¡ ì‹¤í–‰
    print("ğŸ”® ì‹¤í–‰: ë””ë°”ì´ìŠ¤ ê±´ê°• ì˜ˆì¸¡...")
    sample_sensor_data = generate_sample_sensor_data("ESP32-001")
    prediction = await predictor.predict_device_health(sample_sensor_data)
    
    print(f"""
    ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼:
    - ë””ë°”ì´ìŠ¤: {prediction.device_id}
    - ì¥ì•  í™•ë¥ : {prediction.failure_probability:.3f}
    - ì´ìƒ ì ìˆ˜: {prediction.anomaly_score:.3f}
    - ê±´ê°• ì ìˆ˜: {prediction.health_score:.1f}/100
    - ì‹ ë¢°ë„: {prediction.confidence_score:.3f}
    
    ğŸ”§ ê¶Œê³ ì‚¬í•­:
    """)
    
    for i, action in enumerate(prediction.recommended_actions, 1):
        print(f"    {i}. {action}")
    
    print("\nâœ… AI ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ë™ ì™„ë£Œ!")

def generate_sample_training_data(n_samples: int) -> pd.DataFrame:
    """ì˜ˆì‹œ í•™ìŠµ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    
    data = []
    for i in range(n_samples):
        # ì •ìƒ ë°ì´í„° (80%)
        if i < n_samples * 0.8:
            temperature = np.random.normal(25, 5)
            humidity = np.random.normal(60, 15)
            battery_voltage = np.random.normal(3.7, 0.2)
            error_count = np.random.poisson(0.1)
            failure = 0
        # ì´ìƒ ë°ì´í„° (20%)
        else:
            temperature = np.random.normal(40, 10)  # ë†’ì€ ì˜¨ë„
            humidity = np.random.normal(80, 20)     # ë†’ì€ ìŠµë„
            battery_voltage = np.random.normal(3.2, 0.3)  # ë‚®ì€ ë°°í„°ë¦¬
            error_count = np.random.poisson(2)      # ë§ì€ ì—ëŸ¬
            failure = 1
        
        data.append({
            'device_id': f'ESP32-{i%10:03d}',
            'timestamp': datetime.now() - timedelta(hours=i),
            'temperature': temperature,
            'humidity': humidity,
            'pressure': np.random.normal(1013, 20),
            'light_level': np.random.uniform(0, 100),
            'soil_moisture': np.random.uniform(20, 80),
            'battery_voltage': battery_voltage,
            'cpu_usage': np.random.uniform(10, 90),
            'memory_usage': np.random.uniform(20, 80),
            'wifi_signal_strength': np.random.randint(-80, -30),
            'error_count': error_count,
            'uptime_hours': np.random.uniform(1, 168),
            'failure': failure
        })
    
    return pd.DataFrame(data)

def generate_sample_sensor_data(device_id: str) -> List[SensorReading]:
    """ì˜ˆì‹œ ì„¼ì„œ ë°ì´í„° ìƒì„±"""
    readings = []
    base_time = datetime.now()
    
    for i in range(24):  # 24ì‹œê°„ ë°ì´í„°
        reading = SensorReading(
            device_id=device_id,
            timestamp=base_time - timedelta(hours=23-i),
            temperature=25 + np.random.normal(0, 2),
            humidity=60 + np.random.normal(0, 10),
            pressure=1013 + np.random.normal(0, 10),
            light_level=np.random.uniform(0, 100),
            soil_moisture=np.random.uniform(40, 70),
            battery_voltage=3.7 + np.random.normal(0, 0.1),
            cpu_usage=np.random.uniform(20, 60),
            memory_usage=np.random.uniform(30, 70),
            wifi_signal_strength=np.random.randint(-70, -40),
            error_count=np.random.poisson(0.1),
            uptime_hours=i + 1
        )
        readings.append(reading)
    
    return readings

if __name__ == "__main__":
    asyncio.run(main())